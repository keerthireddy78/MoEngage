import nest_asyncio
import asyncio
import csv
from playwright.async_api import async_playwright

nest_asyncio.apply()

BASE_URL = 'https://help.moengage.com'

async def debug_all_links():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            args=[
                "--start-maximized",
                "--disable-blink-features=AutomationControlled"
            ]
        )
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
        )
        page = await context.new_page()
        await page.goto(f'{BASE_URL}/hc/en-us')
        await page.wait_for_load_state('networkidle')

        links = await page.query_selector_all('a')

        print(f"Total <a> tags found: {len(links)}")

        all_hrefs = []
        for link in links:
            href = await link.get_attribute('href')
            if href:
                all_hrefs.append(href)
                print(href)

        await browser.close()
    
    return all_hrefs

all_hrefs = asyncio.get_event_loop().run_until_complete(debug_all_links())

csv_file = 'extracted_links.csv'
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['URL'])  # header
    for href in all_hrefs:
        writer.writerow([href])

print(f"Saved {len(all_hrefs)} links to {csv_file}")
# filtering only the useful links

import csv

input_file = 'extracted_links.csv'
output_file = 'filtered_article_links.csv'

allowed_prefixes = [
    'https://help.moengage.com/hc/en-us/articles/',
    'https://developers.moengage.com/hc/en-us/articles/',
    'https://partners.moengage.com/hc/en-us/articles/'
]

filtered_links = []

with open(input_file, mode='r', encoding='utf-8') as infile:
    reader = csv.DictReader(infile)
    for row in reader:
        url = row['URL']
        if any(url.startswith(prefix) for prefix in allowed_prefixes):
            filtered_links.append(url)

with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:
    writer = csv.writer(outfile)
    writer.writerow(['URL'])  # header
    for link in filtered_links:
        writer.writerow([link])

print(f"Saved {len(filtered_links)} filtered article links to {output_file}")
import asyncio
import nest_asyncio
from playwright.async_api import async_playwright

nest_asyncio.apply()

test_url = 'https://help.moengage.com/hc/en-us/articles/32664799340564-Artificial-Intelligence-Resolution-Agent-AIRA'

async def scrape_single_article(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
        )
        page = await context.new_page()

        print(f"Scraping: {url}")
        await page.goto(url)
        await page.wait_for_load_state('networkidle')

        title_el = await page.query_selector('h6.article-title')
        title = await title_el.inner_text() if title_el else '[No Title Found]'
        print("\nTitle:")
        print(title)

        body_el = await page.query_selector('div.article__body')
        body_text = await body_el.inner_text() if body_el else '[No Body Found]'
        print("\nBody Text (first 500 chars):")
        print(body_text[:500])

        await browser.close()

asyncio.get_event_loop().run_until_complete(scrape_single_article(test_url))
import asyncio
import nest_asyncio
from playwright.async_api import async_playwright

nest_asyncio.apply()

test_url = 'https://help.moengage.com/hc/en-us/articles/32664799340564-Artificial-Intelligence-Resolution-Agent-AIRA'

async def scrape_article_with_sections(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
        )
        page = await context.new_page()

        print(f"Scraping: {url}")
        await page.goto(url)
        await page.wait_for_load_state('networkidle')

        title_el = await page.query_selector('h6.article-title')
        title = await title_el.inner_text() if title_el else '[No Title Found]'
        print(f"\nTitle:\n{title}")

        body_el = await page.query_selector('div.article__body')
        if not body_el:
            print("[No Body Found]")
            await browser.close()
            return

        # full block children
        body_children = await body_el.query_selector_all(':scope > *')

        current_section = "Intro (before first H2)"
        section_content = ""

        for child in body_children:
            tag = await child.evaluate("el => el.tagName.toLowerCase()")

            if tag == 'h2':
                if section_content.strip():
                    print(f"\nSection: {current_section}\n{section_content.strip()}")
                    section_content = ""
                current_section = await child.inner_text()

            else:
                block_text = await child.inner_text()
                if block_text.strip():
                    section_content += block_text + "\n"

                img_elements = await child.query_selector_all('img')
                for img in img_elements:
                    src = await img.get_attribute('src')
                    if src:
                        section_content += f"[Image: {src}]\n"

        if section_content.strip():
            print(f"\nSection: {current_section}\n{section_content.strip()}")

        await browser.close()
asyncio.get_event_loop().run_until_complete(scrape_article_with_sections(test_url))
